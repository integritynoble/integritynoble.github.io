<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-nexty.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-nexty.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"abraham.platformai.org","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Biography                             I am the founder of PlatformAI, dedicated to developing an AI agents platform that anticipates the advent of Artificial General Intelligence (AGI), superintellige">
<meta property="og:type" content="article">
<meta property="og:title" content="Academic Research">
<meta property="og:url" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/index.html">
<meta property="og:site_name" content="Abraham (Chengshuai) Yang">
<meta property="og:description" content="Biography                             I am the founder of PlatformAI, dedicated to developing an AI agents platform that anticipates the advent of Artificial General Intelligence (AGI), superintellige">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/me3.jpg">
<meta property="og:image" content="https://img.shields.io/badge/GitHub-Physics__World__Model-blue?logo=github">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/newa.jpg">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/Beauty_1024%C3%971024.gif">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/Jockey_1024%C3%971024.gif">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/WaterBalloon.gif">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/Dominoes.gif">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/online.png">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/9a.jpg">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/1a.jpg">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/3a.jpg">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/4a.jpg">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/5a.jpg">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/6a.png">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/7a.jpg">
<meta property="og:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/8a.jpg">
<meta property="article:published_time" content="2019-10-23T15:53:24.000Z">
<meta property="article:modified_time" content="2026-02-20T20:17:44.003Z">
<meta property="article:author" content="Abraham (Chengshuai Yang)">
<meta property="article:tag" content="Academic Research">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://abraham.platformai.org/2019/10/23/Scientific-Research/me3.jpg">

<link rel="canonical" href="https://abraham.platformai.org/2019/10/23/Scientific-Research/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Academic Research | Abraham (Chengshuai) Yang</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Abraham (Chengshuai) Yang" type="application/atom+xml">
<link rel="stylesheet" href="\assets\css\APlayer.min.css" class="aplayer-style-marker">
<script src="\assets\js\APlayer.min.js" class="aplayer-script-marker"></script>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Abraham (Chengshuai) Yang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-pwm">

    <a href="/pwm/" rel="section"><i class="fa fa-flask fa-fw"></i>PWM</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    

  <a href="https://github.com/integritynoble" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://abraham.platformai.org/2019/10/23/Scientific-Research/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/me1.jpg">
      <meta itemprop="name" content="Abraham (Chengshuai Yang)">
      <meta itemprop="description" content="With malice toward none, with charity for all, with firmness in the right">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Abraham (Chengshuai) Yang">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Academic Research
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-10-23 08:53:24" itemprop="dateCreated datePublished" datetime="2019-10-23T08:53:24-07:00">2019-10-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2026-02-20 12:17:44" itemprop="dateModified" datetime="2026-02-20T12:17:44-08:00">2026-02-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Academic-Research/" itemprop="url" rel="index"><span itemprop="name">Academic Research</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Biography"><a href="#Biography" class="headerlink" title="Biography"></a>Biography</h1><table>
  <tr>
    <td>
      <img src="/2019/10/23/Scientific-Research/me3.jpg" alt="Image description" style="max-width: 100%;"> <!-- Replace "image_url_here" with the URL of your image -->
    </td><td valign="top"> <!-- This aligns the text vertically -->
      I am the founder of PlatformAI, dedicated to developing an AI agents platform that anticipates the advent of Artificial General Intelligence (AGI), superintelligence, and the singularity—an outlook I share with leading thinkers such as Ray Kurzweil, Elon Musk, Sam Altman, and Leopold Aschenbrenner. To ensure human safety amid rapidly advancing machine intelligence, we are creating the “Doctor of Truth,” a system designed to keep future AI agents aligned with human values. I earned my Ph.D. in June 2020 and subsequently served as a Research Fellow in Machine Learning Optics at Westlake University (October 2020–September 2021) before joining UCLA as a Postdoctoral Researcher in Machine Learning (October 2021–September 2023).
    </td>
 </tr>
      <td style="text-align: center;"> <!-- This centers the text -->
      <a target="_blank" rel="noopener" href="https://twitter.com/@Chengshuai_Yang">Twitter</a> <!-- Replace "your_username" with your Twitter username -->
      <br>
      <a href="integrityyang@gmail.com">integrityyang@gmail.com</a> <!-- Replace with your email address -->      
      </td>      
      <td style="text-align: center;"> <!-- This centers the text -->        
      <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/chengshuai-yang-a14a72bb/">Linkedin</a> <!-- Replace "your_user_id" with your Google Scholar user ID -->
      <br> 
      <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=en&user=ikRGlYcAAAAJ&view_op=list_works&sortby=pubdate">GoogleScholar</a> <!-- Replace "your_user_id" with your Google Scholar user ID -->
      </td>
      
      
      
    
  
</table>


        <div id="aplayer-GUBzXAal" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;">
            <pre class="aplayer-lrc-content"></pre>
        </div>
        <script>
          var ap = new APlayer({
            element: document.getElementById("aplayer-GUBzXAal"),
            narrow: false,
            autoplay: false,
            showlrc: false,
            music: {
              title: "Love",
              author: "Music",
              url: "/images/D571.mp3",
              pic: "http://home.ustc.edu.cn/~mmmwhy/jay.jpg",
              lrc: ""
            }
          });
          window.aplayers || (window.aplayers = []);
          window.aplayers.push(ap);
        </script>

<h1 id="Flagship-Project-Physics-World-Model-PWM"><a href="#Flagship-Project-Physics-World-Model-PWM" class="headerlink" title="Flagship Project: Physics World Model (PWM)"></a>Flagship Project: Physics World Model (PWM)</h1><p><a href="https://github.com/integritynoble/Physics_World_Model/tree/master" target="_blank"><img src="https://img.shields.io/badge/GitHub-Physics__World__Model-blue?logo=github" alt="GitHub Repo"></a>  |  <a href="/pwm/">Full Project Page &amp;rarr;</a></p>
<p><strong>PWM (Physics World Model)</strong> is my flagship open-source project – an evaluation harness and toolkit for computational imaging that makes any imaging system <strong>self-specifying</strong>, <strong>self-diagnosing</strong>, and <strong>self-correcting</strong>. It covers <strong>64 imaging modalities</strong> (from microscopy and medical CT&#x2F;MRI&#x2F;PET to ultrafast photography and neural rendering), <strong>89 graph templates</strong>, and <strong>43+ reconstruction solvers</strong>. The project includes 5 academic papers (targeting venues including Nature), a complete Python package (<code>pwm_core</code>), a companion AI agent bridge (<code>pwm_AI_Scientist</code>), and an adversarial evaluation harness (LIP-Arena). Built around the <strong>OperatorGraph</strong> – a DAG of physics primitives modeling the full imaging pipeline – PWM enables fully reproducible, audit-grade experimental runs with automatic operator calibration and correction.</p>
<p><strong>Key capabilities:</strong> SolveEverything 10-Gear Framework | Physics Fidelity Ladder (4 tiers) | 4-Scenario Evaluation Protocol | CT QC Copilot for clinical diagnostics | Community challenges and CISP 2026</p>
<hr>
<h2 id="Download-CV-x2F-Resume"><a href="#Download-CV-x2F-Resume" class="headerlink" title="Download CV &#x2F; Resume"></a>Download CV &#x2F; Resume</h2><ul>
<li><a href="/CV_Yang.pdf" target="_blank">CV</a> | <a href="/Resume_Chengshuai.pdf" target="_blank">Resume</a></li>
</ul>
<hr>
<p>  With a decade of research experience across a wide range of fields—reinforcement learning, deep learning, unsupervised and self-supervised learning, large language models, AGI, diffusion models, deep unfolding, multimodal optimization, graph neural networks, meta-learning, and advanced imaging techniques—I have authored over 20 publications in top-tier venues such as ECCV, IJCV, Optica, Physical Review Letters, and Physical Review Applied.</p>
<p>At <a target="_blank" rel="noopener" href="https://www.platformai.org/">PlatformAI</a> , we are now focused on building a comprehensive ecosystem of AI influencers and system agents. Our offerings include 24&#x2F;7 AI-powered teachers, a suite of AI tools (e.g., video editors, TTS systems, and image generators), as well as specialized computational imaging solutions for medical and industrial applications. Guided by the core principle of “SpiritAI”—ensuring that agents remain safe, aligned, and subservient to human direction—we envision a future in which humanity transitions from hands-on labor to orchestrating intelligent agents. These personal assistants, copilots, and even company-scale system agents will work under our oversight, fundamentally reshaping the ways we learn, innovate, and thrive.</p>
<h1 id="Work-Experience"><a href="#Work-Experience" class="headerlink" title="Work Experience"></a>Work Experience</h1><h2 id="CEO-and-Cofounder-Full-Time-–-PlatformAI-Los-Angeles-Apr-2024-Now"><a href="#CEO-and-Cofounder-Full-Time-–-PlatformAI-Los-Angeles-Apr-2024-Now" class="headerlink" title="CEO and Cofounder (Full Time)– PlatformAI, Los Angeles	      Apr. 2024-Now"></a>CEO and Cofounder (Full Time)– PlatformAI, Los Angeles	      Apr. 2024-Now</h2><ul>
<li>   Led the machine learning AI influencer project.</li>
<li>Led the automatic computational imaging research.</li>
<li><a target="_blank" rel="noopener" href="https://www.platformai.org/">https://www.platformai.org/</a></li>
</ul>
<h2 id="CEO-and-Cofounder-Half-Time-–-PlatformAI-Los-Angeles-Jun-2023-Mar-2024"><a href="#CEO-and-Cofounder-Half-Time-–-PlatformAI-Los-Angeles-Jun-2023-Mar-2024" class="headerlink" title="CEO and Cofounder (Half Time)– PlatformAI, Los Angeles Jun. 2023-Mar. 2024"></a>CEO and Cofounder (Half Time)– PlatformAI, Los Angeles Jun. 2023-Mar. 2024</h2><ul>
<li>Led the agent chatting app (SpiritAI).</li>
<li>Led the AI tools hub project</li>
</ul>
<h2 id="Machine-Learning-engineering-–-Workmagic-San-FranciscoOct-2023-Mar-2024"><a href="#Machine-Learning-engineering-–-Workmagic-San-FranciscoOct-2023-Mar-2024" class="headerlink" title="Machine Learning engineering  – Workmagic, San Francisco	Oct. 2023-Mar. 2024"></a>Machine Learning engineering  – Workmagic, San Francisco	Oct. 2023-Mar. 2024</h2><h3 id="Led-the-virtual-tryon-project"><a href="#Led-the-virtual-tryon-project" class="headerlink" title="Led the virtual tryon project."></a>Led the virtual tryon project.</h3><ul>
<li>Made the web-UI tryon.</li>
<li>Solved the collar and pants problem of tryon by training detectron2 to get segmentation of cloth.</li>
<li>Trained controlnet to help tryon images get more details.</li>
</ul>
<h3 id="Led-the-diffusion-video-project"><a href="#Led-the-diffusion-video-project" class="headerlink" title="Led the diffusion-video project."></a>Led the diffusion-video project.</h3><ul>
<li>Developed MagicAnimate, ensuring temporal consistency in animations.</li>
<li>Explored DreamPose for fashion image-to-video synthesis using Stable Diffusion.</li>
<li>Formulated an AI-driven video strategy for advertising applications.</li>
</ul>
<h2 id="Postdoctoral-Scholar-in-machine-learning-–-University-of-California-Los-AngelesOct-2021-Sep-2023"><a href="#Postdoctoral-Scholar-in-machine-learning-–-University-of-California-Los-AngelesOct-2021-Sep-2023" class="headerlink" title="Postdoctoral Scholar in machine learning – University of California, Los Angeles	Oct. 2021-Sep. 2023"></a>Postdoctoral Scholar in machine learning – University of California, Los Angeles	Oct. 2021-Sep. 2023</h2><h3 id="Led-the-Large-Language-Models-NLP-project"><a href="#Led-the-Large-Language-Models-NLP-project" class="headerlink" title="Led the Large Language Models (NLP) project."></a>Led the Large Language Models (NLP) project.</h3><ul>
<li>Built a <a target="_blank" rel="noopener" href="https://chatdoctor-4w7fhp6h0-integrityyang-gmailcom.vercel.app/">DoctorBot</a> based on OpenAI API, Langchain and Pinecone.</li>
<li>Medical artificial general intelligence in radiology utilizing a multimodal GPT approach.</li>
<li>Guided multi-model chain of thought reasoning GPT.</li>
<li>Conducted prompt engineering with ChatGPT(3.5 and 4), Bard, Bing, AutoGPT, Dolly, and Vicuna.</li>
<li>Applied LLM methodology to light field tomography (LIFT).</li>
<li>Authored a comprehensive series on Large Language Models (LLMs).</li>
</ul>
<h3 id="Led-the-machine-learning-project-for-light-field-tomography-LIFT"><a href="#Led-the-machine-learning-project-for-light-field-tomography-LIFT" class="headerlink" title="Led the machine learning project for light field tomography (LIFT)."></a>Led the machine learning project for light field tomography (LIFT).</h3><ul>
<li>Achieved practical LIFT reconstruction by combining diffusion models and fine-tuning.</li>
<li>Improved LIFT reconstruction accuracy with GANs in deep unfolding.</li>
<li>Developed unsupervised learning and meta learning for LIFT.</li>
</ul>
<h3 id="Led-the-hyperspectral-imaging-deep-learning-project"><a href="#Led-the-hyperspectral-imaging-deep-learning-project" class="headerlink" title="Led the hyperspectral imaging deep learning project."></a>Led the hyperspectral imaging deep learning project.</h3><ul>
<li>Built the practical reconstruction of hyperspectral imaging by embedding transformer and FastDVDnet into diffusion models based on optimization.</li>
<li>Built a server platform by maintaining Linux systems and managing common Linux accounts based on SSH and VScode.</li>
</ul>
<h3 id="Skills-Cloud-Pytorch-TensorFlow-prompt-engineering-fine-tuning-meta-learning-unsupervised-learning-GAN-diffusion-model-optimization-3D-reconstruction-python-C-x2F-C-Java-SQL"><a href="#Skills-Cloud-Pytorch-TensorFlow-prompt-engineering-fine-tuning-meta-learning-unsupervised-learning-GAN-diffusion-model-optimization-3D-reconstruction-python-C-x2F-C-Java-SQL" class="headerlink" title="Skills: Cloud, Pytorch, TensorFlow, prompt engineering, fine-tuning, meta learning, unsupervised learning, GAN, diffusion model, optimization, 3D reconstruction, python, C&#x2F;C++, Java, SQL."></a>Skills: Cloud, Pytorch, TensorFlow, prompt engineering, fine-tuning, meta learning, unsupervised learning, GAN, diffusion model, optimization, 3D reconstruction, python, C&#x2F;C++, Java, SQL.</h3><h2 id="Machine-learning-optics-team-leader–-Westlake-UniversityOct-2020-Sep-2021"><a href="#Machine-learning-optics-team-leader–-Westlake-UniversityOct-2020-Sep-2021" class="headerlink" title="Machine learning optics team leader– Westlake University	Oct. 2020- Sep. 2021"></a>Machine learning optics team leader– Westlake University	Oct. 2020- Sep. 2021</h2><ul>
<li>Established a machine learning optics team by recruiting PhD students and research assistants based on research background and interviews.</li>
<li>Built a server platform by purchasing five servers with ten GPUs, collaborating with engineers, and maintaining Linux systems.</li>
<li>Conducted reinforcement learning research.</li>
<li>Led the project on ensemble learning for snapshot compressive imaging.<ul>
<li><em>Achieved publication in ECCV 2022 by implementing scalable learning and high-speed strategies.</em><br><img src="/2019/10/23/Scientific-Research/newa.jpg" alt="Scientific-Research"><br><img src="/2019/10/23/Scientific-Research/Beauty_1024%C3%971024.gif" alt="Scientific-Research"><br><img src="/2019/10/23/Scientific-Research/Jockey_1024%C3%971024.gif" alt="Scientific-Research"></li>
</ul>
</li>
<li>Led the project on play and plug for demosaicing snapshot compressive imaging.<ul>
<li><em>Pioneered the online training method to improve play and plug performance, published in the International Journal of Computer Vision (IJCV).</em></li>
</ul>
</li>
<li>Led the project on unsupervised learning for compressed ultrafast photography.</li>
</ul>
<h3 id="Skills-server-Pytorch-Tensorflow-GAN-reinforcement-learning-diffusion-model-optimization-deep-unfolding-demosaicing-3D-reconstruction-unsupervised-learning"><a href="#Skills-server-Pytorch-Tensorflow-GAN-reinforcement-learning-diffusion-model-optimization-deep-unfolding-demosaicing-3D-reconstruction-unsupervised-learning" class="headerlink" title="Skills: server, Pytorch, Tensorflow, GAN, reinforcement learning, diffusion model, optimization, deep unfolding, demosaicing, 3D reconstruction, unsupervised learning."></a>Skills: server, Pytorch, Tensorflow, GAN, reinforcement learning, diffusion model, optimization, deep unfolding, demosaicing, 3D reconstruction, unsupervised learning.</h3><h1 id="Computer-Skills"><a href="#Computer-Skills" class="headerlink" title="Computer Skills"></a>Computer Skills</h1><ul>
<li><strong>Operating system:</strong> Windows, Linux</li>
<li><strong>Programming language:</strong> Python, C, C++, Java, JavaScript, Matlab</li>
<li><strong>Deep learning framework:</strong> Pytorch, Tensorflow, Caffe, AWS, Azure</li>
<li><strong>Others:</strong> Next.js, React, Latex, Langchain, Pinecone, Docker, Sketchup, Hexo, AT89C51 single-chip computer</li>
</ul>
<h1 id="Publication"><a href="#Publication" class="headerlink" title="Publication"></a>Publication</h1><h2 id="Medical-artificial-general-intelligence-in-radiology-utilizing-a-multimodal-GPT-approach-In-progress-2023"><a href="#Medical-artificial-general-intelligence-in-radiology-utilizing-a-multimodal-GPT-approach-In-progress-2023" class="headerlink" title="Medical artificial general intelligence in radiology utilizing a multimodal GPT approach (In progress 2023)"></a>Medical artificial general intelligence in radiology utilizing a multimodal GPT approach (In progress 2023)</h2><p><em>Chengshuai Yang, etc</em></p>
<h2 id="Guided-multi-modal-Chain-of-Thought-reasoning-GPT-In-progress-2023"><a href="#Guided-multi-modal-Chain-of-Thought-reasoning-GPT-In-progress-2023" class="headerlink" title="Guided multi-modal Chain of Thought reasoning GPT (In progress 2023)"></a>Guided multi-modal Chain of Thought reasoning GPT (In progress 2023)</h2><p><em>Chengshuai Yang, etc</em></p>
<h2 id="Ensemble-learning-priors-unfolding-for-scalable-Snapshot-Compressive-Sensing-In-ECCV-2022-link-github"><a href="#Ensemble-learning-priors-unfolding-for-scalable-Snapshot-Compressive-Sensing-In-ECCV-2022-link-github" class="headerlink" title="Ensemble learning priors unfolding for scalable Snapshot Compressive Sensing (In ECCV 2022) link github"></a>Ensemble learning priors unfolding for scalable Snapshot Compressive Sensing (In ECCV 2022) <a target="_blank" rel="noopener" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136830583.pdf">link</a> <a target="_blank" rel="noopener" href="https://github.com/integritynoble/ELP-Unfolding/tree/master">github</a></h2><p><em>Chengshuai Yang, Shiyu Zhang and Xin Yuan</em></p>
<p>Snapshot compressive imaging (SCI) can record the 3D information by a 2D measurement and from this 2D measurement to reconstruct the original 3D information by reconstruction algorithm. As we can see, the reconstruction algorithm plays a vital role in SCI. Recently, deep learning algorithm show its outstanding ability, outperforming the traditional algorithm. Therefore, to improve deep learning algorithm reconstruction accuracy is an inevitable topic for SCI. Besides, deep learning algorithms are usually limited by scalability, and a well trained model in general can not be applied to new systems if lacking the new training process. To address these problems, we develop the ensemble learning priors to further improve the reconstruction accuracy and propose the scalable learning to empower deep learning the scalability just like the traditional algorithm. What’s more, our algorithm has achieved the state-of-the-art results, outperforming existing algorithms. Extensive results on both simulation and real datasets demonstrate the superiority of our proposed algorithm.<img src="/2019/10/23/Scientific-Research/WaterBalloon.gif" alt="Scientific-Research"><br><img src="/2019/10/23/Scientific-Research/Dominoes.gif" alt="Scientific-Research"></p>
<h2 id="Adaptive-Deep-PnP-Algorithm-for-Video-SnapshotCompressive-Imaging-International-Journal-of-Computer-Vision-2023：1-18-link-github"><a href="#Adaptive-Deep-PnP-Algorithm-for-Video-SnapshotCompressive-Imaging-International-Journal-of-Computer-Vision-2023：1-18-link-github" class="headerlink" title="Adaptive Deep PnP Algorithm for Video SnapshotCompressive Imaging  (International Journal of Computer Vision 2023：1-18) link github"></a>Adaptive Deep PnP Algorithm for Video SnapshotCompressive Imaging  (International Journal of Computer Vision 2023：1-18) <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s11263-023-01777-y">link</a> <a target="_blank" rel="noopener" href="https://github.com/xyvirtualgroup/AdaptivePnP_SCI">github</a></h2><p><em>Zongliang Wu#, Chengshuai Yang#, Xiongfei Su and Xin Yuan （#Co-first-author)</em></p>
<p>Video Snapshot compressive imaging (SCI) is a promising technique to capture high-speed videos, which transforms the imaging speed from the detector to mask modulating and only needs a single measurement to capture multiple frames. The algorithm to reconstruct high-speed frames from the measurement plays a vital role in SCI. In this paper, we consider the promising reconstruction algorithm framework, namely plug-and-play (PnP), which is flexible to the encoding process comparing with other deep learning networks. One drawback of existing PnP algorithms is that they use a pre-trained denoising network as a plugged prior while the training data of the network might be different from the task in real applications. Towards this end, in this work, we propose the online PnP algorithm which can adaptively update the network’s parameters within the PnP iteration; this makes the denoising network more applicable to the desired data in the SCI reconstruction. Furthermore, for color video imaging, RGB frames need to be recovered from Bayer pattern or named demosaicing in the camera pipeline. To address this challenge, we design a two-stage reconstruction framework to optimize these two coupled ill-posed problems and introduce a deep demosaicing prior specifically for video demosaicing which does not have much past works instead of using single image demosaicing networks. Extensive results on both simulation and real datasets verify the superiority of our adaptive deep PnP algorithm.<br><img src="/2019/10/23/Scientific-Research/online.png" alt="Scientific-Research"></p>
<h2 id="High-fidelity-image-reconstruction-for-compressed-ultrafast-photograhphy-via-augmented-lagrangian-and-deep-learning-hybrid-algorithm-Photonics-Research-9-2-B30-B37-2021-link-github"><a href="#High-fidelity-image-reconstruction-for-compressed-ultrafast-photograhphy-via-augmented-lagrangian-and-deep-learning-hybrid-algorithm-Photonics-Research-9-2-B30-B37-2021-link-github" class="headerlink" title="High-fidelity image reconstruction for compressed ultrafast photograhphy via augmented-lagrangian and deep-learning hybrid algorithm (Photonics Research 9(2) B30-B37 2021) link github"></a>High-fidelity image reconstruction for compressed ultrafast photograhphy via augmented-lagrangian and deep-learning hybrid algorithm (Photonics Research 9(2) B30-B37 2021) <a target="_blank" rel="noopener" href="https://www.osapublishing.org/prj/fulltext.cfm?uri=prj-9-2-B30&id=446779">link</a> <a target="_blank" rel="noopener" href="https://github.com/integritynoble/ALDL-algorithm">github</a></h2><p><em>Chengshuai Yang, Yunhua Yao, Chengzhi Jin, Dalong Qi, Fengyan Cao, Yilin He, Jiali Yao, Pengpeng Ding, Liang Gao Tianqing Jia1 Jinyang Liang Zhenrong Sun1, and Shian Zhang</em></p>
<p>Compressed ultrafast photography (CUP), which is based on a three-dimensional (3D) image reconstruction method through the compressed sensing (CS) theory, has shown to be a powerful tool in recording self-luminous or non-repeatable ultrafast phenomena. However, the low image fidelity in the conventional augmented-Lagrangian (AL) and two-step iterative shrinkage&#x2F;thresholding (TwIST) algorithms greatly limits the practical applications of CUP, especially for those ultrafast phenomena that need high spatial resolution. Here, we develop a novel AL and deep-learning (DL) hybrid (i.e., AL+DL) algorithm to realize high-fidelity image reconstruction for CUP. The AL+DL algorithm simplifies the mathematical architecture and optimizes the sparse domain and relevant iteration parameters via learning the dataset, and so it improves the image reconstruction accuracy. Our theoretical simulation and experimental results validate the superior performance of the AL+DL algorithm in the image fidelity over the conventional AL and TwIST algorithms, where peak signal to noise ratio (PSNR) and structural similarity index (SSIM) can be increased at least by 4 dB (or 9 dB) and 0.1 (or 0.05) for complex (or simple) dynamic scene, respectively. This study can promote the applications of CUP in related fields, and it will also enable a new strategy for recovering high-dimensional signals from low-dimensional detection. The codes of the AL+DL algorithm are available at <a target="_blank" rel="noopener" href="https://github.com/integritynoble/ALDL-algorithm">https://github.com/integritynoble/ALDL-algorithm</a>.<br><img src="/2019/10/23/Scientific-Research/9a.jpg" alt="Scientific-Research"></p>
<h2 id="Label-free-hyperspectral-imaging-and-deep-learning-prediction-of-retinal-amyloid-β-protein-and-phosphorylated-tau-PNAS-nexus-2022-Sep-1-4-pgac164-link"><a href="#Label-free-hyperspectral-imaging-and-deep-learning-prediction-of-retinal-amyloid-β-protein-and-phosphorylated-tau-PNAS-nexus-2022-Sep-1-4-pgac164-link" class="headerlink" title="Label-free hyperspectral imaging and deep-learning prediction of retinal amyloid β-protein and phosphorylated tau  (PNAS nexus. 2022 Sep;1(4):pgac164) link"></a>Label-free hyperspectral imaging and deep-learning prediction of retinal amyloid β-protein and phosphorylated tau  (PNAS nexus. 2022 Sep;1(4):pgac164) <a target="_blank" rel="noopener" href="https://pubmed.ncbi.nlm.nih.gov/36157597/">link</a></h2><p><em>Xiaoxi Du, Yosef Koronyo, Nazanin Mirzaei, Chengshuai Yang, Dieu-Trang Fuchs, Keith L Black, Maya Koronyo-Hamaoui, Liang Gao</em></p>
<p>Alzheimer’s disease (AD) is a major risk for the aging population. The pathological hallmarks of AD-an abnormal deposition of amyloid β-protein (Aβ) and phosphorylated tau (pTau)-have been demonstrated in the retinas of AD patients, including in prodromal patients with mild cognitive impairment (MCI). Aβ pathology, especially the accumulation of the amyloidogenic 42-residue long alloform (Aβ42), is considered an early and specific sign of AD, and together with tauopathy, confirms AD diagnosis. To visualize retinal Aβ and pTau, state-of-the-art methods use fluorescence. However, administering contrast agents complicates the imaging procedure. To address this problem from fundamentals, ex-vivo studies were performed to develop a label-free hyperspectral imaging method to detect the spectral signatures of Aβ42 and pS396-Tau, and predicted their abundance in retinal cross-sections. For the first time, we reported the spectral signature of pTau and demonstrated an accurate prediction of Aβ and pTau distribution powered by deep learning. We expect our finding will lay the groundwork for label-free detection of AD.</p>
<h2 id="Hyperspectrally-compressed-ultrafast-photography-Physical-Review-Letters-124-023902-2020-link"><a href="#Hyperspectrally-compressed-ultrafast-photography-Physical-Review-Letters-124-023902-2020-link" class="headerlink" title="Hyperspectrally compressed ultrafast photography (Physical Review Letters, 124, 023902 (2020)) link"></a>Hyperspectrally compressed ultrafast photography (Physical Review Letters, 124, 023902 (2020)) <a target="_blank" rel="noopener" href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.124.023902">link</a></h2><p><em>Chengshuai Yang, Fengyan Cao, Dalong Qi, Yilin He, Pengpeng Ding, Jiali Yao ,Tianqing Jia, Zhenrong Sun and Shian Zhang</em></p>
<p>The spatial, temporal and spectral information in optical imaging play a crucial role in exploring unknown world and unencrypting natural mystery. However, the existing optical imaging techniques can only acquire the spatio-temporal or spatio-spectral information of the object with single shot. Here, we develop a hyperspectrally compressed ultrafast photography (HCUP) that can simultaneously record the spatial, temporal and spectral information of the object. In our HCUP, the spatial resolution is 1.26 lp&#x2F;mm in horizontal direction and 1.41 lp&#x2F;mm in vertical direction, the temporal frame interval is 2 ps and the spectral frame interval is 1.72 nm. Moreover, HCUP operates with receive-only and single-shot modes, and therefore it overcomes the technical limitation of active illumination and can measure the non-repetitive or irreversible transient events. Using our HCUP, we successfully measure the spatio-temporal-spectral intensity evolution of the chirped picosecond laser pulse and the photoluminescence dynamics. This study extends the optical imaging from three- to four-dimensional information, which has an important scientific significance in both fundamental research and applied science.<br><img src="/2019/10/23/Scientific-Research/1a.jpg" alt="Scientific-Research"><br><object data="./1.pdf" type="application/pdf" width="100%" height="20%"></object></p>
<h2 id="Optimizing-codes-for-compressed-ultrafast-photography-by-the-genetic-algorithm-Optica-5-2-147-151-2018-link"><a href="#Optimizing-codes-for-compressed-ultrafast-photography-by-the-genetic-algorithm-Optica-5-2-147-151-2018-link" class="headerlink" title="Optimizing codes for compressed ultrafast photography by the genetic algorithm (Optica 5(2), 147-151 (2018))link"></a>Optimizing codes for compressed ultrafast photography by the genetic algorithm (Optica 5(2), 147-151 (2018))<a target="_blank" rel="noopener" href="https://www.osapublishing.org/optica/abstract.cfm?uri=optica-5-2-147">link</a></h2><p><em>Chengshuai Yang, Dalong Qi, Xing Wang, Fengyan Cao, Yilin He, Wenlong Wen, Tianqing Jia, Jinshou Tian, Zhenrong Sun, Liang Gao, Shian Zhang, and Lihong V. Wang.</em> </p>
<p>The compressed ultrafast photography (CUP) technique,providing the fastest receive-only camera so far, has shown to be a well-established tool to capture the ultrafast dynamical scene. This technique is based on random codes to encode and decode the ultrafast dynamical scene by a compressed sensing algorithm. The choice of random codes significantly affects the image reconstruction quality. Therefore, it is important to optimize the encoding codes. Here, we develop a new scheme to obtain the optimized codes by combining a genetic algorithm (GA) into the CUP technique. First, we measure the dynamical scene by the CUP system with random codes and obtain the dynamical scene image at each moment. Second, we use these reconstructed dynamical scene images as the optimization target and optimize the encoding codes based on the GA.Finally we utilize the optimized codes to recapture the dynamicalscene and improve the image reconstruction quality. We validate our optimization scheme by the numerical simulation of a moving double-semielliptical spot and the experimental demonstration of a time- and space-evolving pulsed laser spot.</p>
<h2 id="Single-shot-and-receive-only-ultrafast-electro-optical-deflection-imaging-Physical-Review-Applied-13-024001-2020-link"><a href="#Single-shot-and-receive-only-ultrafast-electro-optical-deflection-imaging-Physical-Review-Applied-13-024001-2020-link" class="headerlink" title="Single-shot and receive-only ultrafast electro-optical deflection imaging (Physical Review Applied, 13, 024001 (2020)) link"></a>Single-shot and receive-only ultrafast electro-optical deflection imaging (Physical Review Applied, 13, 024001 (2020)) <a target="_blank" rel="noopener" href="https://journals.aps.org/prapplied/abstract/10.1103/PhysRevApplied.13.024001">link</a></h2><p><em>Chengshuai Yang, Dalong Qi, Fengyan Cao, Yilin He, Pengpeng Ding, Tianqing Jia, Shixiang Xu, Zhenrong Sun and Shian Zhang</em></p>
<p>Imaging ultrafast dynamic events has been a long-standing scientific goal. It is difficult for conventional electronic imaging sensors based on charge-coupled devices (CCD) or complementary metal-oxide semiconductors (CMOS) to capture dynamic processes occurring on the nanosecond or even shorter time scales due to the limitations of on-chip storage and electronic readout speed. Here, we developed a novel dynamical imaging technique with a very simple, compact configuration called ultrafast electro-optical deflection imaging (UEODI), which has a temporal resolution of up to 20 ps or an imaging speed of 5  1010 frames per second (fps), and therefore it enables observing nanosecond and even picosecond dynamic events. UEODI operates in a single-shot, receive-only mode, and thus it is highly beneficial for imaging non-repetitive (or irreversible) dynamic events and a variety of luminescent objects. Moreover, when combined with the time-of-flight (TOF) method, UEODI can detect three-dimensional (3D) objects. Using UEODI, we visualized a molecular photoluminescent process and measured a 3D ladder structure. Considering the capabilities of UEODI, it will have very importation application prospects in both basic research and applied science, including biomedical imaging.<br><img src="/2019/10/23/Scientific-Research/3a.jpg" alt="Scientific-Research"></p>
<h2 id="Improving-temporal-and-spatial-resolutions-of-compressed-ultrafast-photography-by-using-multi-encoding-imaging-Laser-Physics-Letters-15-11-116202-2018-link"><a href="#Improving-temporal-and-spatial-resolutions-of-compressed-ultrafast-photography-by-using-multi-encoding-imaging-Laser-Physics-Letters-15-11-116202-2018-link" class="headerlink" title="Improving temporal and spatial resolutions of compressed ultrafast photography by using multi-encoding imaging (Laser Physics Letters 15(11), 116202 (2018)) link"></a>Improving temporal and spatial resolutions of compressed ultrafast photography by using multi-encoding imaging (Laser Physics Letters 15(11), 116202 (2018)) <a target="_blank" rel="noopener" href="https://iopscience.iop.org/article/10.1088/1612-202X/aae198/meta">link</a></h2><p><em>Chengshuai Yang, Dalong Qi, Jinyang Liang, Xing Wang, Fengyan Cao, Yilin He, Xiaoping Ouyang, Baoqiang Zhu, Wenlong Wen, Tianqing Jia, Jinshou Tian, Liang Gao, Zhenrong Sun, Shian Zhang, and Lihong V. Wang</em></p>
<p>Imaging ultrafast dynamic scenes has been long pursued by scientists. As a two-dimensional dynamic imaging technique, compressed ultrafast photography (CUP) provides the fastest receive-only camera to capture transient events. This technique is based on three-dimensional image reconstruction by combining streak imaging with compressed sensing (CS). However, the image quality and the frame rate of CUP are limited by the CS-based image reconstruction algorithms and the inherent temporal and spatial resolutions of the streak camera. Here, we report a new method to improve the temporal and spatial resolutions of CUP. Our numerical simulation and experimental verification show that by using a multi-encoding imaging method, both the image quality and the frame rate of CUP can be significantly improved beyond the intrinsic technical parameters. Importantly, the temporal resolution by our scheme can break the limitation of the streak camera. Therefore, this new technology has potential benefits in many applications that require the ultrafast dynamic scene image with high temporal and spatial resolutions.<br><img src="/2019/10/23/Scientific-Research/4a.jpg" alt="Scientific-Research"></p>
<h2 id="Improving-the-image-reconstruction-quality-of-compressed-ultrafast-photography-via-an-augmented-Lagrangian-algorithm-Journal-of-Optics-035703，1-7-2019-link"><a href="#Improving-the-image-reconstruction-quality-of-compressed-ultrafast-photography-via-an-augmented-Lagrangian-algorithm-Journal-of-Optics-035703，1-7-2019-link" class="headerlink" title="Improving the image reconstruction quality of compressed ultrafast photography via an augmented Lagrangian algorithm (Journal of Optics 035703，1-7 (2019)) link"></a>Improving the image reconstruction quality of compressed ultrafast photography via an augmented Lagrangian algorithm (Journal of Optics 035703，1-7 (2019)) <a target="_blank" rel="noopener" href="https://iopscience.iop.org/article/10.1088/2040-8986/ab00d9">link</a></h2><p><em>Chengshuai Yang, Dalong Qi, Fengyan Cao, Yilin He, Xing Wang, Wenlong Wen, Jinshou Tian, Tianqing Jia, Zhenrong Sun and Shian Zhang</em></p>
<p>Compressed ultrafast photography (CUP) has been shown to be a powerful tool to measure ultrafast dynamic scenes. In previous studies, CUP used a two-step iterative shrinkage &#x2F;thresholding (TwIST) algorithm to reconstruct three-dimensional image information. However,the image reconstruction quality greatly depended on the selection of the penalty parameter,which caused the reconstructed images to be unable to be correctly determined if the ultrafast dynamic scenes were unknown in advance. Here, we develop an augmented Lagrangian (AL) algorithm for the image reconstruction of CUP to overcome the limitation of the TwIST algorithm. Our numerical simulations and experimental results show that, compared to the TwIST algorithm, the AL algorithm is less dependent on the selection of the penalty parameter,and can obtain higher image reconstruction quality. This study solves the problem of the image reconstruction instability, which may further promote the practical applications of CUP.<br><img src="/2019/10/23/Scientific-Research/5a.jpg" alt="Scientific-Research"></p>
<h2 id="Compressed-three-dimensional-image-information-and-communication-security-Advanced-Quantum-Technologies-1800034-1-8（2018）-link"><a href="#Compressed-three-dimensional-image-information-and-communication-security-Advanced-Quantum-Technologies-1800034-1-8（2018）-link" class="headerlink" title="Compressed three-dimensional image information and communication security   (Advanced Quantum Technologies 1800034, 1-8（2018）) link"></a>Compressed three-dimensional image information and communication security   (Advanced Quantum Technologies 1800034, 1-8（2018）) <a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/full/10.1002/qute.201800034">link</a></h2><p><em>Chengshuai Yang, Yuyang Ding, Jinyang Liang, Fengyan Cao, Dalong Qi, Tianqing Jia, Zhenrong Sun, Shian Zhang, Wei Chen, Zhenqiang Yin, Shuang Wang, Zhengfu Han, Guangcan Guo, and Lihong V. Wang</em></p>
<p>Ensuring information and communication security in military messages, government instructions, scientific experiments, as well as in personal data processing, is critical. In this study, a new hybrid classical–quantum cryptographic scheme to protect image information and communication security is developed by combining a quantum key distribution (QKD) and compressed sensing (CS). This method employs a QKD system to generate true random codes among the remote legitimate users and utilizes these random codes to encrypt and decrypt compressed 3D image information based on the CS algorithm. Therefore, this new technique can provide computational security in the image information transmission process by the encryption and decryption of CS algorithm, and the information and communication security can be evaluated in real time by monitoring the QKD system. Furthermore, this technique can directly transmit and reconstruct the compressed 3D image information based on the modified TwIST algorithm, and thus fewer random codes are required in QKD system, which can improve the information transmission bandwidth. Consequently, this technique not only provides a new application of a QKD system but also extends the CS-based image reconstruction from 2D to 3D. This study may open a new opportunity in the field of information and security communication.<br><img src="/2019/10/23/Scientific-Research/6a.png" alt="Scientific-Research"></p>
<h2 id="Compressed-ultrafast-electron-diffraction-imaging-through-electronic-encoding-Physical-Review-Applied-054061-10-1-9-2018-link"><a href="#Compressed-ultrafast-electron-diffraction-imaging-through-electronic-encoding-Physical-Review-Applied-054061-10-1-9-2018-link" class="headerlink" title="Compressed ultrafast electron diffraction imaging through electronic encoding (Physical Review Applied 054061(10),1-9 (2018)) link"></a>Compressed ultrafast electron diffraction imaging through electronic encoding (Physical Review Applied 054061(10),1-9 (2018)) <a target="_blank" rel="noopener" href="https://journals.aps.org/prapplied/abstract/10.1103/PhysRevApplied.10.054061">link</a></h2><p><em>Dalong Qi, Chengshuai Yang, Fengyan Cao, Jinyang Liang, Yilin He, Yan Yang, Tianqing Jia, Zhenrong Sun, and Shian Zhang</em></p>
<p>Ultrafast electron diffraction (UED) with high temporal and spatial resolutions is a powerful tool to observe transient structural changes in materials on an atomic scale. This technique is based on a pumpprobe method using ultrashort laser and electron pulses.Therefore,UED requires that the measured transients be highly repeatable. Moreover, the relative time jitter between laser and electron pulses significantly affects the UED temporal resolution. To overcome the UED technical limitations, we propose a technique called compressed ultrafast electron diffraction imaging (CUEDI). In this technique, we encode time-evolving electron diffraction patterns with random codes on an electron encoder. Then,the encoded electron diffraction pattern is measured by a detector after a temporal shearing operation. Finally, the evolution process of the electron diffraction pattern is reconstructed using a compressed sensing algorithm. We confirm the feasibility of our proposed scheme by numerically simulating the polycrystalline goldmelting process based on the experimental data measured with the pump-probe method. Because CUEDI employs a continuous or long electron pulse, the relative time jitter between laser and electron pulses can be eliminated. Additionally, CUEDI measures transients with a single shot, which allows irreversible processes to be directly observed.<br><img src="/2019/10/23/Scientific-Research/7a.jpg" alt="Scientific-Research"></p>
<h2 id="Single-shot-spatiotemporal-intensity-measurement-of-picosecond-laser-pulses-with-compressed-ultrafast-photography-Optics-and-Lasers-in-Engineering-116-89-93-2019-link"><a href="#Single-shot-spatiotemporal-intensity-measurement-of-picosecond-laser-pulses-with-compressed-ultrafast-photography-Optics-and-Lasers-in-Engineering-116-89-93-2019-link" class="headerlink" title="Single-shot spatiotemporal intensity measurement of picosecond laser pulses with compressed ultrafast photography (Optics and Lasers in Engineering 116,89-93 (2019)) link"></a>Single-shot spatiotemporal intensity measurement of picosecond laser pulses with compressed ultrafast photography (Optics and Lasers in Engineering 116,89-93 (2019)) <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0143816618307127">link</a></h2><p><em>Fengyan Cao, Chengshuai Yang, Dalong Qi, Jiali Yao, ilin He, Xing Wang, Wenlong Wen, Jinshou Tian, Tianqing Jia, Zhenrong Sun, and Shian Zhang</em></p>
<p>The spatiotemporal measurement of the ultrashort laser pulses is of great significance in the diagnosis of the instrument performance and the exploration of the laser and matter interaction. In this work we report an advanced compressed ultrafast photography (CUP) technique to measure the spatiotemporal intensity distribution of the picosecond laser pulses with a single shot. This CUP technique is based on a three-dimensional image reconstruction strategy by employing the random codes to encode the space-time-evolving laser pulse and decode it based on a compressed sensing (CS) algorithm. In our CUP system, the measurable laser wavelength depends on the spectral response of the streak camera, which can cover a wide range from ultraviolet (200 nm) to near infrared (850 nm). Based on the CUP system we develop, we successfully measure the spatiotemporal intensity evolutions of some typical laser pulses, such as the 800 nm picosecond laser pulse, the 800 and 400 nm twocolor picosecond laser pulses and the supercontinuum picosecond laser pulse. These experimental results show that the CUP technique can well characterize the spatiotemporal intensity information of the picosecond laser pulses. Moreover, this technique has the remarkable advantages with the single shot measurement and without the reference laser pulse.<br><img src="/2019/10/23/Scientific-Research/8a.jpg" alt="Scientific-Research"></p>
<h2 id="Single-shot-compressed-ultrafast-photography-a-review-Advanced-Photonics-2-1-014003-2020-link"><a href="#Single-shot-compressed-ultrafast-photography-a-review-Advanced-Photonics-2-1-014003-2020-link" class="headerlink" title="Single-shot compressed ultrafast photography: a review (Advanced Photonics, 2(1), 014003(2020)) link"></a>Single-shot compressed ultrafast photography: a review (Advanced Photonics, 2(1), 014003(2020)) <a target="_blank" rel="noopener" href="https://www.spiedigitallibrary.org/journals/advanced-photonics/volume-2/issue-01/014003/Single-shot-compressed-ultrafast-photography-a-review/10.1117/1.AP.2.1.014003.full?SSO=1">link</a></h2><p><em>Dalong Qi, Shian Zhang, Chengshuai Yang, Yilin He, Fengyan Cao, Jiali Yao, Pengpeng Ding, Liang Gao, Tianqing Jia, Zhenrong Sun, Jinyang Liang, and Lihong V.Wang</em></p>
<p>Compressed ultrafast photography (CUP) is a burgeoning single-shot computational imaging technique equipped withthat provides an imaging speed as high as 10 trillion frames per second and a sequence depth of up to a few hundred frames. This technique synergizes compressed sensing and the streak camera technique to capture non-repeatable ultrafast transient events with a single shot. With recent unprecedented technical developments and extensions of this methodology, it has been widely used in ultrafast optical imaging and metrology, ultrafast electron diffraction and microscopy, and information security protection. Here, we review the basic principles of CUP, its recent advances in image acquisition and reconstruction, its fusions with other modalities, and its unique applications in multiple research fields.</p>
<h1 id="Journal-Reviewer"><a href="#Journal-Reviewer" class="headerlink" title="Journal Reviewer"></a>Journal Reviewer</h1><p> Photonics Research: I am a reviewer for Pattern Recognition, ICCV, CVPR and Photonics Research.</p>
<h1 id="Programming-Experience"><a href="#Programming-Experience" class="headerlink" title="Programming Experience"></a>Programming Experience</h1><ul>
<li>Guided multi-modal Chain of Thought reasoning GPT. —-Mar. 2023-</li>
<li>Developed and wrote the supervised diffusion model for hyperspectral imaging. —-Dec. 2022-Mar. 2023</li>
<li>Developed and wrote diffusion model for light field photography. —- Aug. 2022-Nov2022 </li>
<li>Developed and wrote the Generative Adversarial Network (GAN) for light field photography. —-May. 2022- Jul. 2022</li>
<li>Developed and wrote meta learning algorithm for light field photography. —-May. 2022-Jul.2022</li>
<li>Developed and wrote the plug and play algorithm for Compressed ultrafast photography (CUP) —-Jan. 2022 -May. 2022</li>
<li>Developed and wrote the deep unfolding algorithm for CUP FLIM. —-Oct. 2021-Jan. 2022 </li>
<li>Wrote the deep unfolding algorithm codes for video snapshot compressive imaging by python based on pytorch framework. —-April. 2021-Jul. 2021</li>
<li>Wrote the deep reinforcement learning algorithm codes for choosing patch by python based on pytorch framework. —-Dec. 2020-Mar. 2021</li>
<li>Wrote unsupervised learning algorithm codes for CUP. —-Jun. 2020- Dec. 2020</li>
<li>Wrote the augmented lagrangian deep learning algorithm codes. —-Sep. 2019-Jan. 2020</li>
<li>Developed and wrote codes for Hyperspectral CUP. —-Sep. 2019-Jan. 2020</li>
<li>Made a GUI for CUP and wrote augmented lagrangian algorithm codes for CUP. —-Dec. 2018-Jan. 2019</li>
<li>Wrote the program codes of controlling mouse and keyboard to reduce workload by python. —-Mar.2018- Apr. 2018</li>
<li>Developed and wrote genetic algorithm codes for optimizing CUP.  —- Mar. 2017- Jan. 2018</li>
<li>Developed and wrote algorithm codes for compressed ultrafast electron diffraction imaging through electronic encoding. —- Jan.  2017- Oct. 2017</li>
<li>Developed and wrote genetic algorithm codes for compressed three-dimensional image information and communication security.  —- Oct.  2016- Apr. 2017</li>
<li>Developed and wrote two color terahertz codes.  —- Oct.  2015- Mar. 2016</li>
<li>Wrote the program codes of wireless meter reading system in single chip computer by C and Visual Basic, which was my undergraduate thesis and won the first prize in Innovation Competition in Jiangsu Province  —- Mar.2013- Mar.2015</li>
<li>Wrote the program codes of smart home system in single chip computer by C, which won the third prize in Innovation Competition in Jiangsu Province  —- Mar. 2013- Apr.2014</li>
</ul>
<h1 id="Rewards-and-Honors"><a href="#Rewards-and-Honors" class="headerlink" title="Rewards and Honors"></a>Rewards and Honors</h1><ul>
<li>Outstanding Ph.D. in East China Normal University (ECNU).  —- Jun. 2020</li>
<li>Future Scientist Development Program in ECNU.  —- Mar. 2019</li>
<li>Academic Innovation Promotion Program for Excellent Doctoral Students in ECNU. —-Jul. 2018</li>
<li>Outstanding undergraduate in Nantong University —-Jun. 2015</li>
<li>Excellent Undergraduate Thesis in Nantong University —-Jun. 2015</li>
<li>The First Class Scholarship for Undergraduate in Nantong University —-Oct. 2014</li>
<li>Pacemaker to Merit Student in Nantong University —- Oct. 2014</li>
<li>The First Prize of Undergraduate Students’ Physics and Experimental Technology Works<br>Innovation Competition in Jiangsu Province —-Nov. 2013</li>
<li>Computer software level 3 in Jiangsu province —-Dec. 2013</li>
<li>Level 2 Visual Basic of Computer science in Jiangsu Provincial Colleges and Universities —-Dec. 2012</li>
<li>Level 2 C programming language of Computer science in National Colleges and Universities —-Sep. 2013</li>
<li>Level 3 hardware of Computer science in Jiangsu Provincial Colleges and Universities —-May. 2013</li>
<li>Level 3 Software of Computer science in Jiangsu Provincial Colleges and Universities  —- Dec. 2013</li>
<li>The First Prize of Advanced Mathematics Contest in Jiangsu Province —-Jul. 2012</li>
</ul>
<h1 id="Activities"><a href="#Activities" class="headerlink" title="Activities"></a>Activities</h1><ul>
<li>Bibles study in university cooperative housing association, Los Angeles. —-Jun.2022</li>
<li>Future Scientist Development Program in ECNU. —-Mar. 2019-Jan. 2021</li>
<li>Academic Innovation Promotion Program for Excellent Doctoral Students in ECNU. —-Jul. 2018-Jun. 2020</li>
<li>College student training program based on single-chip computer and C programming language in Nantong University. —-Mar. 2013-June. 2014</li>
</ul>
<h1 id="Thank-you-for-your-reading"><a href="#Thank-you-for-your-reading" class="headerlink" title="Thank you for your reading"></a><font color="gray" size="72">Thank you for your reading</font></h1>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Academic-Research/" rel="tag"># Academic Research</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2019/10/26/Pearls-of-Wisdom/" rel="next" title="Pearls of Wisdom">
      Pearls of Wisdom <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Biography"><span class="nav-number">1.</span> <span class="nav-text">Biography</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Flagship-Project-Physics-World-Model-PWM"><span class="nav-number">2.</span> <span class="nav-text">Flagship Project: Physics World Model (PWM)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Download-CV-x2F-Resume"><span class="nav-number">2.1.</span> <span class="nav-text">Download CV &#x2F; Resume</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Work-Experience"><span class="nav-number">3.</span> <span class="nav-text">Work Experience</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#CEO-and-Cofounder-Full-Time-%E2%80%93-PlatformAI-Los-Angeles-Apr-2024-Now"><span class="nav-number">3.1.</span> <span class="nav-text">CEO and Cofounder (Full Time)– PlatformAI, Los Angeles	      Apr. 2024-Now</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CEO-and-Cofounder-Half-Time-%E2%80%93-PlatformAI-Los-Angeles-Jun-2023-Mar-2024"><span class="nav-number">3.2.</span> <span class="nav-text">CEO and Cofounder (Half Time)– PlatformAI, Los Angeles Jun. 2023-Mar. 2024</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Machine-Learning-engineering-%E2%80%93-Workmagic-San-FranciscoOct-2023-Mar-2024"><span class="nav-number">3.3.</span> <span class="nav-text">Machine Learning engineering  – Workmagic, San Francisco	Oct. 2023-Mar. 2024</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Led-the-virtual-tryon-project"><span class="nav-number">3.3.1.</span> <span class="nav-text">Led the virtual tryon project.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Led-the-diffusion-video-project"><span class="nav-number">3.3.2.</span> <span class="nav-text">Led the diffusion-video project.</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Postdoctoral-Scholar-in-machine-learning-%E2%80%93-University-of-California-Los-AngelesOct-2021-Sep-2023"><span class="nav-number">3.4.</span> <span class="nav-text">Postdoctoral Scholar in machine learning – University of California, Los Angeles	Oct. 2021-Sep. 2023</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Led-the-Large-Language-Models-NLP-project"><span class="nav-number">3.4.1.</span> <span class="nav-text">Led the Large Language Models (NLP) project.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Led-the-machine-learning-project-for-light-field-tomography-LIFT"><span class="nav-number">3.4.2.</span> <span class="nav-text">Led the machine learning project for light field tomography (LIFT).</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Led-the-hyperspectral-imaging-deep-learning-project"><span class="nav-number">3.4.3.</span> <span class="nav-text">Led the hyperspectral imaging deep learning project.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Skills-Cloud-Pytorch-TensorFlow-prompt-engineering-fine-tuning-meta-learning-unsupervised-learning-GAN-diffusion-model-optimization-3D-reconstruction-python-C-x2F-C-Java-SQL"><span class="nav-number">3.4.4.</span> <span class="nav-text">Skills: Cloud, Pytorch, TensorFlow, prompt engineering, fine-tuning, meta learning, unsupervised learning, GAN, diffusion model, optimization, 3D reconstruction, python, C&#x2F;C++, Java, SQL.</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Machine-learning-optics-team-leader%E2%80%93-Westlake-UniversityOct-2020-Sep-2021"><span class="nav-number">3.5.</span> <span class="nav-text">Machine learning optics team leader– Westlake University	Oct. 2020- Sep. 2021</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Skills-server-Pytorch-Tensorflow-GAN-reinforcement-learning-diffusion-model-optimization-deep-unfolding-demosaicing-3D-reconstruction-unsupervised-learning"><span class="nav-number">3.5.1.</span> <span class="nav-text">Skills: server, Pytorch, Tensorflow, GAN, reinforcement learning, diffusion model, optimization, deep unfolding, demosaicing, 3D reconstruction, unsupervised learning.</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Computer-Skills"><span class="nav-number">4.</span> <span class="nav-text">Computer Skills</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Publication"><span class="nav-number">5.</span> <span class="nav-text">Publication</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Medical-artificial-general-intelligence-in-radiology-utilizing-a-multimodal-GPT-approach-In-progress-2023"><span class="nav-number">5.1.</span> <span class="nav-text">Medical artificial general intelligence in radiology utilizing a multimodal GPT approach (In progress 2023)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Guided-multi-modal-Chain-of-Thought-reasoning-GPT-In-progress-2023"><span class="nav-number">5.2.</span> <span class="nav-text">Guided multi-modal Chain of Thought reasoning GPT (In progress 2023)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ensemble-learning-priors-unfolding-for-scalable-Snapshot-Compressive-Sensing-In-ECCV-2022-link-github"><span class="nav-number">5.3.</span> <span class="nav-text">Ensemble learning priors unfolding for scalable Snapshot Compressive Sensing (In ECCV 2022) link github</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adaptive-Deep-PnP-Algorithm-for-Video-SnapshotCompressive-Imaging-International-Journal-of-Computer-Vision-2023%EF%BC%9A1-18-link-github"><span class="nav-number">5.4.</span> <span class="nav-text">Adaptive Deep PnP Algorithm for Video SnapshotCompressive Imaging  (International Journal of Computer Vision 2023：1-18) link github</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#High-fidelity-image-reconstruction-for-compressed-ultrafast-photograhphy-via-augmented-lagrangian-and-deep-learning-hybrid-algorithm-Photonics-Research-9-2-B30-B37-2021-link-github"><span class="nav-number">5.5.</span> <span class="nav-text">High-fidelity image reconstruction for compressed ultrafast photograhphy via augmented-lagrangian and deep-learning hybrid algorithm (Photonics Research 9(2) B30-B37 2021) link github</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Label-free-hyperspectral-imaging-and-deep-learning-prediction-of-retinal-amyloid-%CE%B2-protein-and-phosphorylated-tau-PNAS-nexus-2022-Sep-1-4-pgac164-link"><span class="nav-number">5.6.</span> <span class="nav-text">Label-free hyperspectral imaging and deep-learning prediction of retinal amyloid β-protein and phosphorylated tau  (PNAS nexus. 2022 Sep;1(4):pgac164) link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hyperspectrally-compressed-ultrafast-photography-Physical-Review-Letters-124-023902-2020-link"><span class="nav-number">5.7.</span> <span class="nav-text">Hyperspectrally compressed ultrafast photography (Physical Review Letters, 124, 023902 (2020)) link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizing-codes-for-compressed-ultrafast-photography-by-the-genetic-algorithm-Optica-5-2-147-151-2018-link"><span class="nav-number">5.8.</span> <span class="nav-text">Optimizing codes for compressed ultrafast photography by the genetic algorithm (Optica 5(2), 147-151 (2018))link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Single-shot-and-receive-only-ultrafast-electro-optical-deflection-imaging-Physical-Review-Applied-13-024001-2020-link"><span class="nav-number">5.9.</span> <span class="nav-text">Single-shot and receive-only ultrafast electro-optical deflection imaging (Physical Review Applied, 13, 024001 (2020)) link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improving-temporal-and-spatial-resolutions-of-compressed-ultrafast-photography-by-using-multi-encoding-imaging-Laser-Physics-Letters-15-11-116202-2018-link"><span class="nav-number">5.10.</span> <span class="nav-text">Improving temporal and spatial resolutions of compressed ultrafast photography by using multi-encoding imaging (Laser Physics Letters 15(11), 116202 (2018)) link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Improving-the-image-reconstruction-quality-of-compressed-ultrafast-photography-via-an-augmented-Lagrangian-algorithm-Journal-of-Optics-035703%EF%BC%8C1-7-2019-link"><span class="nav-number">5.11.</span> <span class="nav-text">Improving the image reconstruction quality of compressed ultrafast photography via an augmented Lagrangian algorithm (Journal of Optics 035703，1-7 (2019)) link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Compressed-three-dimensional-image-information-and-communication-security-Advanced-Quantum-Technologies-1800034-1-8%EF%BC%882018%EF%BC%89-link"><span class="nav-number">5.12.</span> <span class="nav-text">Compressed three-dimensional image information and communication security   (Advanced Quantum Technologies 1800034, 1-8（2018）) link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Compressed-ultrafast-electron-diffraction-imaging-through-electronic-encoding-Physical-Review-Applied-054061-10-1-9-2018-link"><span class="nav-number">5.13.</span> <span class="nav-text">Compressed ultrafast electron diffraction imaging through electronic encoding (Physical Review Applied 054061(10),1-9 (2018)) link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Single-shot-spatiotemporal-intensity-measurement-of-picosecond-laser-pulses-with-compressed-ultrafast-photography-Optics-and-Lasers-in-Engineering-116-89-93-2019-link"><span class="nav-number">5.14.</span> <span class="nav-text">Single-shot spatiotemporal intensity measurement of picosecond laser pulses with compressed ultrafast photography (Optics and Lasers in Engineering 116,89-93 (2019)) link</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Single-shot-compressed-ultrafast-photography-a-review-Advanced-Photonics-2-1-014003-2020-link"><span class="nav-number">5.15.</span> <span class="nav-text">Single-shot compressed ultrafast photography: a review (Advanced Photonics, 2(1), 014003(2020)) link</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Journal-Reviewer"><span class="nav-number">6.</span> <span class="nav-text">Journal Reviewer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Programming-Experience"><span class="nav-number">7.</span> <span class="nav-text">Programming Experience</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Rewards-and-Honors"><span class="nav-number">8.</span> <span class="nav-text">Rewards and Honors</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Activities"><span class="nav-number">9.</span> <span class="nav-text">Activities</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Thank-you-for-your-reading"><span class="nav-number">10.</span> <span class="nav-text">Thank you for your reading</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Abraham (Chengshuai Yang)"
      src="/images/me1.jpg">
  <p class="site-author-name" itemprop="name">Abraham (Chengshuai Yang)</p>
  <div class="site-description" itemprop="description">With malice toward none, with charity for all, with firmness in the right</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/integritynoble" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;integritynoble" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:integrityyang@gmail.com" title="E-Mail → mailto:integrityyang@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2026</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Abraham (Chengshuai Yang)</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  















  

  

  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/love.js"></script>
</body>
</html>
